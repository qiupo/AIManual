# 代码生成与编写

AI最强大的能力之一是生成代码。学会如何高效地让AI生成高质量代码，能极大提升开发效率。

## 🎯 本章目标

- ✅ 掌握各类代码的生成技巧
- ✅ 学会不同场景的提示词策略
- ✅ 理解如何优化AI生成的代码
- ✅ 建立代码生成的最佳实践

## 📋 代码生成类型

### 类型1: 函数和方法

**场景**: 需要实现特定功能

```
提示词模板：
"实现一个[编程语言]的[功能描述]函数

要求：
- 输入：[输入参数说明]
- 输出：[输出说明]
- 功能：[详细功能描述]
- 边界情况：[需要处理的情况]
- 性能要求：[如有]
- 添加详细注释

请提供：
1. 完整代码实现
2. 使用示例
3. 时间复杂度分析
4. 可能的改进建议"
```

**实战示例**：

```
提示词：
"用Python实现一个LRU缓存装饰器

要求：
- 使用 functools.wraps
- 支持 maxsize 参数
- 线程安全
- 添加详细注释
- 提供使用示例

实现后解释：
1. LRU原理
2. OrderedDict的作用
3. 线程安全的实现方式"
```

**AI输出**：
```python
from functools import wraps
from collections import OrderedDict
import threading

def lru_cache(maxsize=128):
    """
    LRU (Least Recently Used) 缓存装饰器

    参数：
        maxsize: 最大缓存数量，默认128

    原理：
        1. 使用 OrderedDict 保持访问顺序
        2. 访问时移动到末尾（最新）
        3. 满时删除最旧的项（首项）
    """
    def decorator(func):
        cache = OrderedDict()
        lock = threading.Lock()

        @wraps(func)
        def wrapper(*args, **kwargs):
            # 创建可哈希的键
            key = (args, frozenset(kwargs.items()))

            with lock:
                if key in cache:
                    # 命中：移到末尾
                    cache.move_to_end(key)
                    return cache[key]

                # 未命中：计算结果
                result = func(*args, **kwargs)

                # 存入缓存
                cache[key] = result

                # 超出容量：删除最旧的
                if len(cache) > maxsize:
                    cache.popitem(last=False)

                return result

        return wrapper
    return decorator

# 使用示例
@lru_cache(maxsize=100)
def fibonacci(n):
    """计算斐波那契数列（带缓存）"""
    if n < 2:
        return n
    return fibonacci(n-1) + fibonacci(n-2)

# 测试
print(fibonacci(100))  # 快速计算，因为有缓存
```

### 类型2: 类和模块

**场景**: 需要完整的类实现

```
提示词模板：
"创建一个[编程语言]类，实现[功能]

类的职责：
1. [职责1]
2. [职责2]
3. [职责3]

要求：
- 使用[设计模式]
- 包含[方法列表]
- 异常处理
- 类型注解（如适用）
- 文档字符串
- 单元测试示例"
```

**实战示例**：

```
提示词：
"用Python创建一个DatabaseManager类

功能：
- 支持多种数据库（PostgreSQL, MySQL, SQLite）
- 连接池管理
- 自动重连
- 查询结果缓存
- 事务管理
- 日志记录

要求：
- 使用上下文管理器
- 完整的错误处理
- 类型注解
- 详细文档
- 使用示例"
```

### 类型3: API和Web服务

**场景**: 创建RESTful API或Web服务

```
提示词模板：
"创建一个[框架]的API服务

功能：[API功能描述]

技术栈：
- 框架：[Flask/FastAPI/Django等]
- 数据库：[数据库类型]
- 认证：[认证方式]

要求：
1. RESTful设计
2. 输入验证
3. 错误处理
4. API文档
5. 测试用例
6. 部署配置"
```

**实战示例**：

```
提示词：
"用FastAPI创建一个任务管理API

端点设计：
1. POST /tasks - 创建任务
2. GET /tasks - 获取所有任务
3. GET /tasks/{id} - 获取单个任务
4. PUT /tasks/{id} - 更新任务
5. DELETE /tasks/{id} - 删除任务

数据模型：
- title: string
- description: string
- status: pending/completed
- created_at: datetime
- updated_at: datetime

要求：
- 使用 Pydantic 模型
- SQLAlchemy + PostgreSQL
- JWT认证
- 请求验证
- 完整的错误处理
- API文档（Swagger）
- Docker配置"
```

### 类型4: 数据处理脚本

**场景**: 数据清洗、转换、分析

```
提示词模板：
"写一个脚本处理[数据类型]

任务：
1. [步骤1]
2. [步骤2]
3. [步骤3]

输入格式：[格式说明]
输出格式：[格式说明]

要求：
- 错误处理
- 进度显示
- 日志记录
- 性能优化
- 单元测试"
```

**实战示例**：

```
提示词：
"写一个Python脚本处理CSV文件

任务：
1. 读取多个CSV文件
2. 合并数据
3. 去重（基于ID列）
4. 填充缺失值
5. 数据验证
6. 输出处理后的CSV

要求：
- 使用 pandas
- 命令行参数（文件路径）
- 处理大文件（分块）
- 详细日志
- 处理报告
- 异常处理
- 进度条"

示例数据结构：
- id: 整数
- name: 字符串
- email: 字符串
- created_at: 日期"
```

**AI输出**：
```python
import pandas as pd
import argparse
import logging
from pathlib import Path
from tqdm import tqdm
import sys

# 配置日志
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

def process_csv_files(file_paths, output_path):
    """
    处理多个CSV文件

    参数：
        file_paths: CSV文件路径列表
        output_path: 输出文件路径
    """
    all_data = []
    stats = {
        'total_rows': 0,
        'duplicates_removed': 0,
        'missing_values_filled': 0
    }

    # 1. 读取文件
    logger.info(f"开始处理 {len(file_paths)} 个文件")
    for file_path in tqdm(file_paths, desc="读取文件"):
        try:
            # 分块读取大文件
            chunks = pd.read_csv(file_path, chunksize=10000)
            for chunk in chunks:
                all_data.append(chunk)
                stats['total_rows'] += len(chunk)
        except Exception as e:
            logger.error(f"读取文件 {file_path} 失败: {e}")

    if not all_data:
        logger.error("没有读取到任何数据")
        return

    # 2. 合并数据
    logger.info("合并数据...")
    df = pd.concat(all_data, ignore_index=True)

    # 3. 去重
    logger.info("去重处理...")
    before_dedup = len(df)
    df = df.drop_duplicates(subset=['id'], keep='last')
    stats['duplicates_removed'] = before_dedup - len(df)

    # 4. 填充缺失值
    logger.info("填充缺失值...")
    # 数值列用中位数
    numeric_columns = df.select_dtypes(include=['number']).columns
    for col in numeric_columns:
        missing_count = df[col].isna().sum()
        if missing_count > 0:
            df[col].fillna(df[col].median(), inplace=True)
            stats['missing_values_filled'] += missing_count

    # 字符串列用 'Unknown'
    string_columns = df.select_dtypes(include=['object']).columns
    for col in string_columns:
        missing_count = df[col].isna().sum()
        if missing_count > 0:
            df[col].fillna('Unknown', inplace=True)
            stats['missing_values_filled'] += missing_count

    # 5. 数据验证
    logger.info("验证数据...")
    # 检查必需列
    required_columns = ['id', 'name', 'email']
    missing_columns = set(required_columns) - set(df.columns)
    if missing_columns:
        logger.error(f"缺少必需列: {missing_columns}")
        return

    # 验证邮箱格式
    email_pattern = r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}$'
    invalid_emails = ~df['email'].str.match(email_pattern, na=False)
    if invalid_emails.any():
        logger.warning(f"发现 {invalid_emails.sum()} 条无效邮箱记录")

    # 6. 保存结果
    logger.info(f"保存结果到 {output_path}")
    df.to_csv(output_path, index=False)

    # 7. 打印报告
    logger.info("=" * 50)
    logger.info("处理完成！统计信息：")
    logger.info(f"  总行数: {stats['total_rows']}")
    logger.info(f"  最终行数: {len(df)}")
    logger.info(f"  删除重复: {stats['duplicates_removed']}")
    logger.info(f"  填充缺失值: {stats['missing_values_filled']}")
    logger.info("=" * 50)

def main():
    parser = argparse.ArgumentParser(description='处理多个CSV文件')
    parser.add_argument('files', nargs='+', help='CSV文件路径')
    parser.add_argument('-o', '--output', required=True, help='输出文件路径')
    args = parser.parse_args()

    # 验证输入文件
    file_paths = [Path(f) for f in args.files]
    for path in file_paths:
        if not path.exists():
            logger.error(f"文件不存在: {path}")
            sys.exit(1)

    process_csv_files(file_paths, args.output)

if __name__ == '__main__':
    main()
```

### 类型5: 算法实现

**场景**: 实现特定算法或数据结构

```
提示词模板：
"实现[算法名称]算法

要求：
- [编程语言]
- 详细注释
- 时间/空间复杂度分析
- 优化建议
- 测试用例
- 可视化（如适用）"
```

**实战示例**：

```
提示词：
"用Python实现以下排序算法：
1. 快速排序
2. 归并排序
3. 堆排序

对每个算法：
1. 完整实现
2. 详细注释
3. 时间复杂度分析
4. 空间复杂度分析
5. 适用场景
6. 性能测试
7. 可视化对比"
```

### 类型6: 测试代码

**场景**: 为现有代码编写测试

```
提示词模板：
"为以下代码编写测试：

[粘贴代码]

测试框架：[pytest/unittest/Jest等]

要求：
1. 单元测试
2. 边界测试
3. 异常测试
4. 集成测试
5. 测试覆盖率 > 90%
6. 使用 fixtures/mocks"
```

**实战示例**：

```
提示词：
"为这个User类编写完整的测试套件

[粘贴User类代码]

使用pytest，测试：
1. 创建用户
2. 验证邮箱
3. 密码哈希
4. 更新信息
5. 删除用户
6. 边界情况
7. 异常处理

要求：
- 使用 pytest fixtures
- 参数化测试
- Mock外部依赖
- 测试覆盖率报告"
```

## 🎨 代码生成技巧

### 技巧1: 渐进式生成

对于复杂功能，分步骤生成：

```
步骤1: 生成基础框架
"给我一个Flask应用的基础结构"

步骤2: 添加特定功能
"在上面的基础上，添加用户认证功能"

步骤3: 完善细节
"为认证功能添加JWT token管理"
```

### 技巧2: 风格一致性

```
提示词：
"写一个Python函数，风格如下：

[粘贴你已有的代码示例]

保持相同的：
- 命名规范
- 注释风格
- 错误处理方式
- 代码结构"
```

### 技巧3: 性能优化

```
提示词：
"这是我的代码：
[粘贴代码]

它运行太慢了，请帮我优化：
1. 分析性能瓶颈
2. 提供优化方案
3. 给出优化后代码
4. 对比性能提升
5. 说明权衡"
```

### 技巧4: 安全加固

```
提示词：
"审查这段代码的安全性：
[粘贴代码]

检查：
1. SQL注入
2. XSS攻击
3. CSRF保护
4. 敏感数据泄露
5. 权限检查

提供：
1. 安全问题列表
2. 修复建议
3. 安全版本代码"
```

## 📊 代码质量检查清单

AI生成代码后，检查：

### ✅ 功能性
- [ ] 实现了所有需求
- [ ] 处理了边界情况
- [ ] 错误处理完善

### ✅ 性能
- [ ] 时间复杂度合理
- [ ] 没有明显性能问题
- [ ] 考虑了大数据量

### ✅ 安全性
- [ ] 输入验证
- [ ] SQL注入防护
- [ ] XSS防护
- [ ] 敏感信息保护

### ✅ 可维护性
- [ ] 代码结构清晰
- [ ] 命名有意义
- [ ] 注释充分
- [ ] 符合DRY原则

### ✅ 测试
- [ ] 包含测试用例
- [ ] 覆盖主要场景
- [ ] 测试边界条件

## ⚡ 实战练习

### 练习1: 基础功能
```
让AI帮你写一个函数：
"实现一个通用的二分查找函数，
支持自定义比较函数，
添加详细注释和使用示例"
```

### 练习2: 完整类
```
让AI创建一个类：
"实现一个ConfigManager类，
支持从JSON/YAML/ENV读取配置，
支持环境变量覆盖，
支持配置验证"
```

### 练习3: API服务
```
让AI创建API：
"实现一个短链接服务API，
包括创建、重定向、统计功能，
使用FastAPI + Redis"
```

### 练习4: 数据处理
```
让AI处理数据：
"写一个脚本，
监控指定文件夹，
当有新文件时自动处理并归档"
```

## 💡 最佳实践

### DO ✅

1. **明确需求**
   - 详细描述功能
   - 说明技术栈
   - 列出约束条件

2. **分步骤**
   - 复杂任务分解
   - 逐步完善功能
   - 每步验证结果

3. **审查代码**
   - 理解每一行
   - 检查潜在问题
   - 运行测试验证

4. **持续改进**
   - 让AI优化代码
   - 添加更多功能
   - 改进错误处理

### DON'T ❌

1. **不要盲目复制**
   - 必须理解代码
   - 检查安全性
   - 验证正确性

2. **不要跳过测试**
   - AI代码也要测试
   - 边界情况很重要
   - 性能要验证

3. **不要忽视规范**
   - 遵循代码规范
   - 保持一致性
   - 添加文档

## 小结

- AI能生成各种类型的代码
- 掌握不同场景的提示词技巧
- 渐进式生成复杂功能
- 始终审查和测试生成代码
- 代码质量比开发速度更重要

## 下一步

学会了代码生成，让我们学习[代码调试与优化](./03-代码调试与优化.md)。

---

💡 **记住**：AI生成的代码是起点，不是终点。理解、测试、优化才能用于生产环境！
